"""
Intelligent Systems 1
Week 10 Lab Sheet
Dr James Stovold
This week’s “lab” sheet is a little different. As the week 10 material consists of philosophical and
ethical questions around the development and use of artificial intelligence, I want you to write an
essay (500–750 words) on one of the topics listed below. Then, in week 10, there will be an
opportunity to peer review another essay from the cohort, providing you with the opportunity to
consider another person’s perspective on the questions, and providing you with feedback on
yours.
Remember to include appropriate academic references to any external material that you use.
Topics:
• Should robots have rights?
• If computers can learn, who is responsible when they go wrong?
• If the brain is essentially mechanical, what implications does this have for strong AI?
• How will the (predominantly white male) tech industry perpetuate bias through the
development of AI systems?
If you want to be included in the peer review process, please fill out this form before Thursday
week 9: https://forms.gle/cpmMbhQjUJ2uhUWo8
"""


# Attempt 1, mildly late : was peer reviewed
"""
INT1:- Week 10 Lab Sheet

Should robots have rights?



There are many arguments that may influence the question of whether or not robots should be given rights, from what could happen if we give them too much freedom - so much so that their programming logic works against our favour (since they aren’t subject to the same morality as humans like to think they are, fluctuating and subjective though it may be), to how the bias that is inbuilt in them is exhibited. There may be various intentions behind the arguments at play as well, it may be in some interests to continue to exploit robotic workforces for their own financial gains. Could laws take into account every situation that may occur? No, but better to have them as a safety net than not.



The only way we can infer if a being understands something is to observe their behaviour while testing them (beetle in box). We are willing to accept this method of testing understanding in human society, but why are we so reluctant to accept this for machines?



There are arguments for strong and weak AI, but does that warrant them having rights per se? It could be argued if they are thinking beings as Strong AI could suggest, they deserve to be treated like humans or animals. If we can build them, can we treat them correctly? However does thinking and computing equate to feeling? Personally, I would say the one which would cause the greatest sway would be linked to the questioning of their being capable of feeling/holding emotions. Do they hold a will to learn and evolve and survive like other life forms do as we know it?  Are they subject to whims and desires? Or are we, programming them to do so, influencing that? Does thought prove life and existence? Does the proving of their being able to behave intelligently mean that they have ability to.
"""

# peer review for attempt 1

"""
Peer review template


Author Name: Abir Rizwanullah (kr995)
Reviewer Name: James Stovold (jhs503)
Title of Essay: Should robots have rights?

First and foremost – read the rubric! You were asked to submit a pdf or docx file, not paste the essay into the VLE submission. 

ORGANIZATION
Were the basic sections (Introduction, Conclusion, Literature Cited, etc.) adequate? If not, what is missing? N/A
Did the writer use subheadings well to clarify the sections of the text? Explain. No, N/A.
Was the material ordered in a way that was logical, clear, easy to follow? Explain.

CITATIONS
Did the writer cite sources adequately and appropriately? Note any incorrect formatting.
No sources cited
Were all the citations in the text listed in the Literature Cited section? Note any discrepancies.
As above.

GRAMMAR AND STYLE
Were there any grammatical or spelling problems?
Some minor grammatical errors (dangling prepositions etc.)
Was the writer's writing style clear? Were the paragraphs and sentences cohesive?
Writer’s style was reasonable, flowed well, but reads as though it is a stream of consciousness rather than a structured argument.

CONTENT
Did the writer adequately summarize and discuss the topic? Explain.
No, writer raises some interesting questions, but does not appear to answer any of them.
List three strengths of the work. Why are they strengths? Explain.
The writer clearly understands that there are many issues in this area 
The writer does a good job of highlighting core questions that need answering
Good use of hypophoria, although the answers could do with more depth.
List three weaknesses of the work. Why are they weaknesses? Explain.
Essay is quite short (508 words), so plenty of space to expand their thinking. As such, the questions raised are very high level, and do not probe the ethical problems around this area sufficiently.
I think you would have been better off picking a specific example, case study, or ethical dilemma (any one of the questions you raised would be sufficient) and focussed on that rather than trying to cover everything in 500-750 words.
No reference to source material, other than a throwaway comment about Wittgenstein’s private language argument (uncited). If this was an assessment, this would border on academic misconduct / plagiarism. 
Did the writer comprehensively cover appropriate materials available from the standard sources? If no, what's missing?
No citations
Did the writer make some contribution of thought to the paper, or merely summarize data or publications? Explain.
No citations, so difficult to tell.
"""

# ohhhhh, this is why he was mildly miffed at what he got
"""
Name: Abir Rizwanullah (kr995)
Assignment: Submission point for essay on philosophical/ethical issues
Date Submitted: Monday, 30 November 2020 09:00:40 o'clock GMT
Current Mark: Needs Marking

Submission Field:
<p dir="ltr" style="text-align: center;margin-top: 0.0pt;margin-bottom: 0.0pt;"><span style="font-size: 16.5pt;font-family: Roboto , sans-serif;color: rgb(32,33,36);background-color: rgb(255,255,255);font-weight: 400;font-style: normal;font-variant: normal;text-decoration: none;vertical-align: baseline;">INT1:- Week 10 Lab Sheet</span></p> 
<p dir="ltr" style="text-align: center;margin-top: 0.0pt;margin-bottom: 0.0pt;"><span style="font-size: 16.5pt;font-family: Roboto , sans-serif;color: rgb(32,33,36);background-color: rgb(255,255,255);font-weight: bold;font-style: normal;font-variant: normal;text-decoration: none;vertical-align: baseline;">Should robots have rights?</span></p> 
<p><b style="font-weight: normal;"> </b></p> 
<p dir="ltr" style="margin-top: 0.0pt;margin-bottom: 0.0pt;"><span style="font-size: 12.0pt;font-family: Arial;color: rgb(32,33,36);background-color: rgb(255,255,255);font-weight: 400;font-style: normal;font-variant: normal;text-decoration: none;vertical-align: baseline;">There are many arguments that may influence the question of whether or not robots should be given rights, from what could happen if we give them too much freedom - so much so that their programming logic works against our favour (since they aren’t subject to the same morality as humans like to think they are, fluctuating and subjective though it may be), to how the bias that is inbuilt in them is exhibited. There may be various intentions behind the arguments at play as well, it may be in some interests to continue to exploit robotic workforces for their own financial gains. Could laws take into account every situation that may occur? No, but better to have them as a safety net than not.</span></p> 
<p><b style="font-weight: normal;"> </b></p> 
<p dir="ltr" style="margin-top: 0.0pt;margin-bottom: 0.0pt;"><span style="font-size: 12.0pt;font-family: Arial;color: rgb(32,33,36);background-color: rgb(255,255,255);font-weight: 400;font-style: normal;font-variant: normal;text-decoration: none;vertical-align: baseline;">The only way we can infer if a being understands something is to observe their behaviour while testing them (beetle in box). We are willing to accept this method of testing understanding in human society, but why are we so reluctant to accept this for machines?</span></p> 
<p><b style="font-weight: normal;"> </b></p> 
<p dir="ltr" style="margin-top: 0.0pt;margin-bottom: 0.0pt;"><span style="font-size: 12.0pt;font-family: Arial;color: rgb(32,33,36);background-color: rgb(255,255,255);font-weight: 400;font-style: normal;font-variant: normal;text-decoration: none;vertical-align: baseline;">There are arguments for strong and weak AI, but does that warrant them having rights per se? It could be argued if they are thinking beings as Strong AI could suggest, they deserve to be treated like humans or animals. If we can build them, can we treat them correctly? However does thinking and computing equate to feeling? Personally, I would say the one which would cause the greatest sway would be linked to the questioning of their being capable of feeling/holding emotions. Do they hold a will to learn and evolve and survive like other life forms do as we know it?  Are they subject to whims and desires? Or are we, programming them to do so, influencing that? Does thought prove life and existence? Does the proving of their being able to behave intelligently mean that they have ability to.</span></p> 
<p><b style="font-weight: normal;"></b></p>

Comments:
There are no student comments for this assignment.

Files:
No files were attached to this submission."""


# Attempt 2, very late
"""
INT1:- Week 10 Lab Sheet
Should robots have rights? by Abir
Even the prospect of humans entertaining the notion of being able to create Artificial
Intelligence with consciousness - i.e. life - begs the question if they could even
handle the consequences fairly. Do we even have the capacity to be meddling in
such affairs, and are we able to cope with the various consequences of it all
regardless of how unprecedented or preimagined they could be? Could we treat AI
right? How would just ways of treating them be implemented? It is also important to
bear in mind that there are many arguments that may influence the question of
whether or not (or to what extent) robots should be given rights, from what could
happen if we give them too much freedom - so much so that their programming logic
works against our favour (since they are not subject to the same morality constructs
as humans like to think they are, fluctuating and subjective though it may be), to how
the bias that is inbuilt in them is exhibited and will unfairly affect lives in practice.
There may be various intentions behind the arguments at play as well, it may be in
some interests to exploit (robotic) workforces (skewing public opinion by making
them seem less ‘alive’/’human’/’relatable’ regardless of actual, proven consciousness
- as has awfully happened in human history) for their own financial gains, and in
others, to oppose technological progress at the expense of unemployment due to
automation replacing human activity, and in turn destroying livelihoods. In this essay,
I shall ramble on about the perceptions that I personally feel will hold the most sway
on an overwhelming majority of humans’s consciences (or so I optimistically hope)
when it comes to deciding upon the extent at which AI should have rights, if at all.
The Strong AI (stating that a computer that acts intelligently is actually intelligent) vs
Weak AI debate (stating that a computer that acts intelligently is just mimicking
intelligent behaviour and is not actually intelligent) may also be another factor in
association with the ethics concerned with AI. The former takes the stance of if
something is behaving like it is intelligent, it is considered intelligent. There is the
Chinese Room argument in opposition to Strong AI, but how can human
understanding - which is usually taken as fact - be proven in the first place, let alone
that of computers? The standard way of ‘learning’ for both humans and AI is by
applying a method again and again until it becomes second nature to carry it out - it
is not innate. The standard way one can infer if a being ‘understands’ something is to
observe their behaviour while testing them. This is the measure humans adopt when
testing for understanding on all levels in human society, but they are quite reluctant
to accept this method of testing for intelligence in machines. It could be argued if
they are indeed thinking beings, as concepts surrounding Strong AI suggest, they
deserve to be treated like humans or animals are.
But, for argument’s sake, if robots do/shall indeed have sentience. What would they
be getting out of it? Rights as payment for their responsibilities for the greater good
of human societal preservation (but everyone has to play part in society, even if we
harness maybe conscious robots to work towards our own society). Responsibilities
that may have been imposed upon them and are not of their own volition. Then
again, since when did humans care so much: humans harness many an other
being’s potential for their own human society’s progress without said being’s
consent, which could be classified as unethical - but, if you take sympathy out of the
equation, selfpreservation takes precedence and it is survival of the fittest.
However, does thinking and computing equate to feeling? Do they hold a will to learn
and evolve and survive like other life forms do as we know them to? Are they - or will
they ever become - subject to their own whims and desires, like a drive to succeed
outside of their own responsibilities? Or are we, programming them to do so,
influencing that false semblance of motivation that wouldn’t exist without? How does
one define a being as sentient? Is it if they have a ‘lifeforce’? What influences the
spark of ‘life’, usually proliferated by propagation, that is so difficult to prove or
disprove, but is so very intuitive and believed in by some. Does thought and the
ability to sense prove life and existence (see: René Descartes’s “I think, therefore I
am” and “Senses can lie or manipulate but to know doubt is to have thoughts and
thoughts lead to existence”)? Does the proving of their being able to behave
intelligently mean that they have the capacity to be curious and want to emulate role
models - to learn from and adapt teachings into our own lives?
Mental thought processes can indeed be encompassed by functions. But, does the
ability to think actually imply consciousness per se? A consciousness is what
actually allows one to grow and learn and take from it instead of just carrying out a
mathematical process by rote. Depends on our definition of consciousness. But what
if we shift the way we define sentience and perceiving and reacting, from something
that arises through an intelligent computational process, to something more abstract
and innate and illogical, something comprised of feelings and sympathies and
attachments. Thing is, humans and animals do not only think logically. If you think of
Freud’s psychoanalytic theory, there are three components that make up the
structural model of the psyche: the instinctual id, the moral superego and the rational
ego. Of all of these, robotic computation can be thought to mimic the ego the most,
but what of the other two conceptual components. We know that animals are subject
to primal instincts, but morality is said to be an instinct shaped by societal factors
specific to humans. Then again, this is just another model of the human mind that is
a mere abstraction and has its flaws. The implication is not that we should provide
rights (or degrees of) based solely on evidence if one has the capacity to feel or not
(nor to what extent), but it should be taken into consideration as it may influence
some people’s judgements.
Consciousness is not a computation in itself, as Roger Penrose puts it, but perhaps
there is room for consciousness to arise by computation. Will there come a time
when we observe AI form attachments and exhibit signs of feeling sorrow, pain, joy
etc? But will it be real… How could we ethically test the trueness of their emotions.
As callous as it sounds, does it logically matter if they cannot feel the injustices being
carried out against them? Being held to do something not subject of their own will,
even if that will is absent. Will it not be a lot of wasted time and effort if we go about
giving them rights if it turns out they don’t even feel? But how truly disdainfully
superior is it of us to go about debating another’s capacity to feel (see: beetle in box
thought experiment - can we be sure if anyone feels the same ~ anything). It would
be easier on the conscience to have rights in place regardless of evidence. Easier
said than done though. What human in their right mind would bet on an uncertain
outcome, just in probable case of something grievous or apocalyptic happening in
the seemingly far future. But if we are wrong, we will have added another notch to
the discording history of humankind. Is it not better to prevent a grave possible
outcome that had been proposed beforehand, than to have to attempt to cure the
after effects after the consequences - regardless of believability.
On another note, could ethics and laws even take into account every situation that
may occur? No, but better to have some degree of a safety net than not. Legislation
should thoroughly document all the proceedings to avoid things going horribly wrong
(or document them going horribly wrong). Even if one is skeptical of the opinion that
machines are intelligent and sympathetic and sentient, they should have rights.
Perhaps not in exactly the same way humans and animals do, for there are
differences between animal rights and human rights - according to fundamental
differences in needs (and varying human benevolence). It is good that we have laws
in place for technology, however these are undermined and loopholes are found
around them all too often. To have laws in place for the technology we create though
is a different kettle of fish. If an entity contributes to society, they should have their
proper place in society. Their rights unto society and society’s rights unto them
should be defined in a contract, regardless of perceived sentience, to make sure no
unfairness on that entity is carried out by the state, or vice versa. However, this could
be exploited, for example by people with malicious intent against particular
individuals or groups. Another thing to consider is would we attribute their
achievements and profits to only them or their creators? Or even other people
involved. If we look at most industries, for example the publishing industry,
copyrights can last for well after the deaths of original content creators, so it would
be difficult to see much changing there.
Oh well, we won’t know, until we know. Not everything can be predicted or go wholly
well, and it won’t be perfect. But until then, we can at least try to be prepared for the
most outlandish thing, and strive for the least worst possibility. Better to be prepared
for the worst as opposed to winging it when it does come to pass, as opposed to not
being prepared for anything at all {ahem - pandemic}. Personally, I would say the
argument which would hold the greatest weight in their having rights would be linked
to the questioning of their being capable of having feelings induced in them, as
opposed to their being able to process thought. However, the question of whether to
have rights for robots should not be totally dependent on this. And so I, with my own
frankly narrow inconsequential view, conclude that it is how we implement giving
rights that matters, and that it would be nice if all entities that a society is composed
of have documentation of fair contracts between them and their society in all matters,
so that no party is unjustly treated.  # loooooool what on earth was I on about X'D
"""